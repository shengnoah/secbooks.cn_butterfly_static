{"title":"视频生成模型：构建虚拟世界的模拟器","uid":"8acb019eb4d4e800260b2c7eda03a5c4","slug":"paper/视频生成模型：构建虚拟世界的模拟器","date":"2024-03-16T11:44:51.910Z","updated":"2024-03-16T11:44:51.910Z","comments":true,"path":"api/articles/paper/视频生成模型：构建虚拟世界的模拟器.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":null,"content":"<h1 id=\"视频生成模型构建虚拟世界的模拟器\"><a class=\"markdownIt-Anchor\" href=\"#视频生成模型构建虚拟世界的模拟器\"></a> 视频生成模型：构建虚拟世界的模拟器</h1>\n<p>翻译了OpenAI关于Sora相关的技术报告：《Video generation models as world simulators | 视频生成模型：构建虚拟世界的模拟器》</p>\n<p>这篇技术报告主要介绍了两方面内容：(1) OpenAI如何将各种类型的视觉数据转化为统一的表示形式，从而实现生成模型的大规模训练；(2) 对 Sora 模型能力和局限性的定性评价。</p>\n<p>报告中没有包含模型和实施的详细信息。</p>\n<p>Sora 属于扩散型 Transformer（diffusion transformer）。</p>\n<p>我们知道，传统的 Transformer，主要有Encoder和Decoder，Encoder是将文本编码成 Token，从而可以将自然语言变成可以统一处理的数字或代码。而 Decoder 则是将 Token 反向解码成文本。</p>\n<p>而 Sora 也是类似的思路，只不过它编码的结果不是Token，报告里面叫 Patches（中文暂译做补片），Encoder 将视频压缩为低维潜空间，再将其分解为 Patches。同样 Sora 也能从 Patches 反向解码成视频图像。（参考图一）</p>\n<p>Sora 同时还是一种扩散模型，能将有噪声的图像块，基于 Prompt 还原出清晰的图像。（参考图二）</p>\n<p>另外，报告中特地提到了：“我们的研究显示，扩展视频生成模型的规模是向着创建能够模拟物理世界的通用工具迈出的有前途的一步。”</p>\n<p>据说微软前一段时间给OpenAI搞了五千亿个视频用于训练。</p>\n<p>原文：</p>\n<p><a href=\"https://openai.com/research/video-generation-models-as-world-simulators?continueFlag=2c6d842b1332338a289515b10241db55\">https://openai.com/research/video-generation-models-as-world-simulators?continueFlag=2c6d842b1332338a289515b10241db55</a></p>\n","text":" 视频生成模型：构建虚拟世界的模拟器 翻译了OpenAI关于Sora相关的技术报告：《Video generation models as world simulators | 视频生成模型：构建虚拟世界的模拟器》 这篇技术报告主要介绍了两方面内容：(1) OpenAI如何将各种...","link":"","photos":[],"count_time":{"symbolsCount":772,"symbolsTime":"1 mins."},"categories":[{"name":"AIGC","slug":"AIGC","count":119,"path":"api/categories/AIGC.json"},{"name":"weibo","slug":"AIGC/weibo","count":59,"path":"api/categories/AIGC/weibo.json"}],"tags":[{"name":"weibo","slug":"weibo","count":62,"path":"api/tags/weibo.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E8%99%9A%E6%8B%9F%E4%B8%96%E7%95%8C%E7%9A%84%E6%A8%A1%E6%8B%9F%E5%99%A8\"><span class=\"toc-text\"> 视频生成模型：构建虚拟世界的模拟器</span></a></li></ol>","author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"用于多模式文档理解的布局感知生成语言模型","uid":"fdd0a7bc582d8610e53c9b2f7cf8673b","slug":"paper/用于多模式文档理解的布局感知生成语言模型","date":"2024-03-16T11:44:51.910Z","updated":"2024-03-16T11:44:51.910Z","comments":true,"path":"api/articles/paper/用于多模式文档理解的布局感知生成语言模型.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":[],"text":" 用于多模式文档理解的布局感知生成语言模型 DocLLM: A layout-aware generative language model for multimodal document understanding （用于多模式文档理解的布局感知生成语言模型） 论文：arxiv...","link":"","photos":[],"count_time":{"symbolsCount":597,"symbolsTime":"1 mins."},"categories":[{"name":"AIGC","slug":"AIGC","count":119,"path":"api/categories/AIGC.json"},{"name":"weibo","slug":"AIGC/weibo","count":59,"path":"api/categories/AIGC/weibo.json"}],"tags":[{"name":"weibo","slug":"weibo","count":62,"path":"api/tags/weibo.json"}],"author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"Python显示request请求的响应时间戳","uid":"02ac3d01366c7a352094c19df12c098f","slug":"python/Python显示request请求的响应时间戳","date":"2024-03-16T11:44:51.910Z","updated":"2024-03-16T11:44:51.911Z","comments":true,"path":"api/articles/python/Python显示request请求的响应时间戳.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":null,"text":" Python显示request请求的响应时间戳 Python显示request请求的响应时间戳 要显示Python的request请求的响应时间戳，可以使用time模块来获取请求开始和结束的时间，并计算它们之间的差异。以下是一个示例代码： 1234567891011121314...","link":"","photos":[],"count_time":{"symbolsCount":"3.1k","symbolsTime":"3 mins."},"categories":[{"name":"AIGC","slug":"AIGC","count":119,"path":"api/categories/AIGC.json"},{"name":"python","slug":"AIGC/python","count":1,"path":"api/categories/AIGC/python.json"}],"tags":[{"name":"python","slug":"python","count":2,"path":"api/tags/python.json"}],"author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}}}}