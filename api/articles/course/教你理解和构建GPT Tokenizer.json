{"title":"教你理解和构建GPT Tokenizer","uid":"3f78d971eabe4d648c246dd6ba6afdb2","slug":"course/教你理解和构建GPT Tokenizer","date":"2024-03-17T14:44:56.873Z","updated":"2024-03-16T11:44:51.785Z","comments":true,"path":"api/articles/course/教你理解和构建GPT Tokenizer.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":[],"content":"<h1 id=\"教你理解和构建gpt-tokenizer\"><a class=\"markdownIt-Anchor\" href=\"#教你理解和构建gpt-tokenizer\"></a> 教你理解和构建GPT Tokenizer</h1>\n<p>Open AI传奇研究员Andrej Karpathy的新课，教你理解和构建GPT Tokenizer。</p>\n<p>他可以把相当复杂的LLM概念用非常好理解的方式讲出来。希望了解LLM的强烈建议听一下他的课，包括一些历史课程。</p>\n<p>用GPT-4翻译了一下这节课，感兴趣可以听一下。字幕文件下载和历史课程会放最后。</p>\n<p>补充一下视频介绍：</p>\n<p>分词器是大语言模型（LLM）处理流程中一个独立且关键的环节。它们有专属的训练数据集、采用特定的训练算法——字节对编码（Byte Pair Encoding），训练完成后，分词器能够执行两个核心功能：encode() 函数将普通文本字符串转换为词元，而 decode() 函数则能将词元还原为原始文本字符串。在这场讲座中，我们将一步步揭开 OpenAI GPT 系列分词器的构建过程。</p>\n<p>我们将发现，许多大语言模型(LLM)表现出的异常行为和问题，其实都源于标记化(tokenization)这一环节。我们会针对这些问题进行详细讨论，探究标记化为何成为问题的关键所在，以及为什么最理想的情况是有人能够找到办法，完全去除这一处理阶段。</p>\n<p>00:00:00 intro: Tokenization, GPT-2 paper, tokenization-related issues<br />\n00:05:50 tokenization by example in a Web UI (tiktokenizer)<br />\n00:14:56 strings in Python, Unicode code points<br />\n00:18:15 Unicode byte encodings, ASCII, UTF-8, UTF-16, UTF-32<br />\n00:22:47 daydreaming: deleting tokenization<br />\n00:23:50 Byte Pair Encoding (BPE) algorithm walkthrough<br />\n00:27:02 starting the implementation<br />\n00:28:35 counting consecutive pairs, finding most common pair<br />\n00:30:36 merging the most common pair<br />\n00:34:58 training the tokenizer: adding the while loop, compression ratio<br />\n00:39:20 tokenizer/LLM diagram: it is a completely separate stage<br />\n00:42:47 decoding tokens to strings<br />\n00:48:21 encoding strings to tokens<br />\n00:57:36 regex patterns to force splits across categories<br />\n01:11:38 tiktoken library intro, differences between GPT-2/GPT-4 regex<br />\n01:14:59 GPT-2 <a href=\"http://encoder.py\">encoder.py</a> released by OpenAI walkthrough<br />\n01:18:26 special tokens, tiktoken handling of, GPT-2/GPT-4 differences<br />\n01:25:28 minbpe exercise time! write your own GPT-4 tokenizer<br />\n01:28:42 sentencepiece library intro, used to train Llama 2 vocabulary<br />\n01:43:27 how to set vocabulary set? revisiting <a href=\"http://gpt.py\">gpt.py</a> transformer<br />\n01:48:11 training new tokens, example of prompt compression<br />\n01:49:58 multimodal [image, video, audio] tokenization with vector quantization<br />\n01:51:41 revisiting and explaining the quirks of LLM tokenization<br />\n02:10:20 final recommendations</p>\n<p><a href=\"https://pan.quark.cn/s/60d51adb8ecc#/list/share\">https://pan.quark.cn/s/60d51adb8ecc#/list/share</a></p>\n<p><img src=\"https://gitee.com/shengnoah/picture/raw/master/20240221124153.png\" alt=\"image.png\" /></p>\n","text":" 教你理解和构建GPT Tokenizer Open AI传奇研究员Andrej Karpathy的新课，教你理解和构建GPT Tokenizer。 他可以把相当复杂的LLM概念用非常好理解的方式讲出来。希望了解LLM的强烈建议听一下他的课，包括一些历史课程。 用GPT-4翻译了...","link":"","photos":[],"count_time":{"symbolsCount":"1.9k","symbolsTime":"2 mins."},"categories":[{"name":"课程","slug":"课程","count":1,"path":"api/categories/课程.json"}],"tags":[{"name":"GPT","slug":"GPT","count":1,"path":"api/tags/GPT.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E6%95%99%E4%BD%A0%E7%90%86%E8%A7%A3%E5%92%8C%E6%9E%84%E5%BB%BAgpt-tokenizer\"><span class=\"toc-text\"> 教你理解和构建GPT Tokenizer</span></a></li></ol>","author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"chromadb","uid":"f265caf366478add089d1a27b17487ff","slug":"database/chromadb","date":"2024-03-17T14:44:56.881Z","updated":"2024-03-16T11:44:51.787Z","comments":true,"path":"api/articles/database/chromadb.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":null,"text":" chromadb chromadb是一种什么样的数据库，在什么场景使用chroma数据库。 ChromaDB是一种高性能的时间序列数据库，主要针对存储和分析大规模的时间序列数据。它被设计用于处理和查询实时数据，例如传感器数据、日志数据、度量指标等。 ChromaDB适用于许多场...","link":"","photos":[],"count_time":{"symbolsCount":"3.2k","symbolsTime":"3 mins."},"categories":[{"name":"database","slug":"database","count":3,"path":"api/categories/database.json"},{"name":"vector","slug":"database/vector","count":1,"path":"api/categories/database/vector.json"}],"tags":[{"name":"chromadb","slug":"chromadb","count":1,"path":"api/tags/chromadb.json"}],"author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"HFish蜜罐与SOC安全运营中心","uid":"3e5e0ad875e008f2f5596d741d0ac42f","slug":"candylab/soc","date":"2024-03-17T14:44:56.866Z","updated":"2024-03-16T11:44:51.785Z","comments":true,"path":"api/articles/candylab/soc.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":[],"text":" HFish蜜罐与SOC安全运营中心 Excerpt 作者：糖果 0x01 传统蜜罐 传统蜜罐在安全运营当中，起到防御与威胁发现的作用。蜜罐系统提供Web（WordPress等）服务模拟、及各种主机服务模拟，比如：ElasticSearch、FTP、Telnet、Redis等。 ...","link":"","photos":[],"count_time":{"symbolsCount":"2.6k","symbolsTime":"2 mins."},"categories":[{"name":"文章","slug":"文章","count":22,"path":"api/categories/文章.json"}],"tags":[],"author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}}}}